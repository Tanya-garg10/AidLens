# ğŸ§  AidLens â€“ AI for Social Good  
**Powered by Google Gemini**

AidLens is a multimodal AI assistant designed to help NGOs, volunteers, and students understand complex information and take responsible next steps. It supports **text, images, PDFs, and voice input**, making critical information easier to access and act upon in real-world social impact scenarios.

## ğŸŒ Why AidLens?

In social good and community work, important information often comes in confusing formatsâ€”dense text messages, scanned documents, PDFs, images, or even spoken instructions. Misunderstanding such information can cause delays or unsafe decisions.  
AidLens bridges this gap by converting information from multiple formats into **clear, structured, and actionable guidance**.

## âœ¨ Key Features

- ğŸ“ **Text Input** â€“ Paste messages or notes for instant explanation  
- ğŸ–¼ï¸ **Image Upload** â€“ Understand scanned notices or document images  
- ğŸ“„ **PDF Support** â€“ Extract and analyze content from multi-page PDFs  
- ğŸ¤ **Voice Input (Mic + Upload)** â€“ Speak directly or upload audio for transcription and analysis  
- ğŸ‘¥ **Role-Based Responses** â€“ Tailored outputs for NGO Workers, Volunteers, and Students  
- ğŸ“‹ **Structured Output** â€“ Summary, key points, next steps, risks, and clarifying questions  
- âš ï¸ **Responsible AI Design** â€“ Safety-aware responses for sensitive topics  

## ğŸ› ï¸ Built With

- **Python** â€“ Core programming language  
- **Google Gemini API** â€“ Multimodal reasoning, content generation, and audio transcription  
- **Google AI Studio** â€“ API key management and model access  
- **Streamlit** â€“ Web application framework  
- **Streamlit Cloud** â€“ Hosting and public demo deployment  
- **Pillow (PIL)** â€“ Image processing  
- **pdfplumber** â€“ PDF text extraction  
- **streamlit-audiorec** â€“ Microphone-based voice input  
- **Prompt Engineering** â€“ Structured, role-based, and safe AI responses  
- **GitHub** â€“ Version control and open-source collaboration  

## âš™ï¸ How It Works

1. User provides input via text, image, PDF, or voice  
2. Voice inputs are transcribed using Gemini  
3. PDF text is extracted and combined with other inputs  
4. Gemini processes the content using multimodal reasoning  
5. AidLens returns a structured and role-specific response  

## â–¶ï¸ Run Locally

### 1. Clone the repository
```bash
git clone https://github.com/your-username/aidlens.git
cd aidlens
````

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set your Gemini API key

```bash
export GEMINI_API_KEY=your_api_key_here
```

*(or enter it in the app sidebar)*

### 4. Run the app

```bash
streamlit run app.py
```

## âš ï¸ Disclaimer

AidLens is an informational support tool and does not replace professional medical, legal, or emergency advice. Users should consult qualified professionals when necessary.

## ğŸš€ Future Roadmap

* Multilingual support for regional languages
* Text-to-speech output for accessibility
* Deeper customization for NGO workflows
* Improved handling of handwritten and large PDFs

## ğŸ’™ Impact

AidLens demonstrates how advanced multimodal AI can be applied responsibly beyond chat interfacesâ€”supporting clarity, accessibility, and real-world decision-making for social good.

